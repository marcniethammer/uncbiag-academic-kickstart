@inproceedings{DBLP:conf/icml/HoferGNK20,
 abstract = {We study regularization in the context of small sample-size learning with over-parameterized neural networks. Specifically, we shift focus from architectural properties, such as norms on the network weights, to properties of the internal representations before a linear classifier. Specifically, we impose a topological constraint on samples drawn from the probability measure induced in that space. This provably leads to mass concentration effects around the representations of training instances, ie, a property beneficial for generalization. By leveraging previous work to impose topological constraints in a neural network setting, we provide empirical evidence (across various vision benchmarks) to support our claim for better generalization.},
 author = {Christoph D. Hofer and
Florian Graf and
Marc Niethammer and
Roland Kwitt},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/HoferGNK20.bib},
 booktitle = {Proceedings of the 37th International Conference on Machine Learning,
ICML 2020, 13-18 July 2020, Virtual Event},
 keywords = {ICML,deep learning,topology},
 pages = {4304--4313},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Tue, 15 Dec 2020 17:40:19 +0100},
 title = {Topologically Densified Distributions},
 url = {http://proceedings.mlr.press/v119/hofer20a.html},
 volume = {119},
 year = {2020}
}

